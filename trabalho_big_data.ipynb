{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comandos para realização do trabalho da matéria de Big Data com uso da biblioteca PySpark.\n",
    "\n",
    "## <font color=red>Observação importante:</font>\n",
    "\n",
    "<font color=yellow>Trabalho realizado com uso da biblioteca pandas não será aceito!</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload do arquivo `imdb-reviews-pt-br.csv` para dentro do Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/N-CPUninter/Big_Data/main/data/imdb-reviews-pt-br.zip -O imdb-reviews-pt-br.zip\n",
    "!unzip imdb-reviews-pt-br.zip\n",
    "!rm imdb-reviews-pt-br.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalação manual das dependências para uso do pyspark no Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar, instanciar e criar a SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "appName = \"PySpark Trabalho de Big Data\"\n",
    "master = \"local\"\n",
    "\n",
    "spark = SparkSession.builder.appName(appName).master(master).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criar spark dataframe do CSV utilizando o método read.csv do spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_df = spark.read.csv('imdb-reviews-pt-br.csv', \n",
    "                         header=True, \n",
    "                         quote=\"\\\"\", \n",
    "                         escape=\"\\\"\", \n",
    "                         encoding=\"UTF-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questão 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criar funções de MAP:\n",
    "- Criar função para mapear o \"sentiment\" como chave e o \"id\" como valor do tipo inteiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map1(x):\n",
    "  # Coloque aqui o seu código para retornar a tupla necessária.\n",
    "  # Apague a linha abaixo para iniciar seu código.\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cria funções de REDUCE:\n",
    "\n",
    "- Criar função de reduce para somar os IDs por \"sentiment\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduceByKey1(x,y):\n",
    "  # Coloque aqui o seu código para retornar o resultado necessário.\n",
    "  # Apague a linha abaixo para iniciar seu código.\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicação do map/reduce e visualização do resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coloque aqui a sua linha de código para aplicar o map/reduce no seu \n",
    "# dataframe spark e realize o collect() ao final para visualizar os dados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questão 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criar funções de MAP:\n",
    "- Criar função para mapear o \"sentiment\" como chave e uma tupla com a soma das palavras de cada texto como valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map2(x):\n",
    "  # Coloque aqui o seu código para retornar a tupla necessária.\n",
    "  # Apague a linha abaixo para iniciar seu código.\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cria funções de REDUCE:\n",
    "\n",
    "- Criar função de reduce para somar o numero de palavras de cada texto português e inglês por \"sentiment\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduceByKey2(x,y):\n",
    "  # Coloque aqui o seu código para retornar o resultado necessário.\n",
    "  # Apague a linha abaixo para iniciar seu código.\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicação do map/reduce e visualização do resultado\n",
    "\n",
    "1. Aplicar o map/reduce no seu dataframe spark e realizar o collect() ao final\n",
    "2. Selecionar os dados referentes aos textos negativos para realizar a subtração.\n",
    "3. Realizar a subtração das contagens de palavras dos textos negativos para obter o resultado final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coloque aqui suas linhas de código final\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
