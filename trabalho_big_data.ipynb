{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comandos para realiza√ß√£o do trabalho da mat√©ria de Big Data com uso da biblioteca PySpark.\n",
    "\n",
    "## <font color=red>Observa√ß√£o importante:</font>\n",
    "\n",
    "<font color=yellow>Trabalho realizado com uso da biblioteca pandas n√£o ser√° aceito!</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload do arquivo `imdb-reviews-pt-br.csv` para dentro do Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded imdb-reviews-pt-br.zip successfully!\n",
      "Extracted files from imdb-reviews-pt-br.zip\n",
      "Removed imdb-reviews-pt-br.zip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/N-CPUninter/Big_Data/main/data/imdb-reviews-pt-br.zip\"\n",
    "filename = \"imdb-reviews-pt-br.zip\"\n",
    "\n",
    "response = requests.get(url, stream=True)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    with open(filename, \"wb\") as f:\n",
    "        for chunk in response.iter_content(1024):\n",
    "            f.write(chunk)\n",
    "    print(f\"Downloaded {filename} successfully!\")\n",
    "else:\n",
    "    print(f\"Error downloading file: {response.status_code}\")\n",
    "\n",
    "with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "    zip_ref.extractall()\n",
    "    print(\"Extracted files from\", filename)\n",
    "\n",
    "os.remove(filename)\n",
    "print(f\"Removed {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar, instanciar e criar a SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando spark session para Raquel - RU 3803786\n",
      "Sess√£o iniciado com sucesso! üöÄ\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "MEU_RU = \"3803786\"\n",
    "appName = f\"PySpark Trabalho de Big Data - {MEU_RU}\"\n",
    "master = \"local\"\n",
    "\n",
    "print(f\"Iniciando spark session para Raquel - RU {MEU_RU}\")\n",
    "spark: SparkSession = SparkSession.builder.appName(appName).master(master).getOrCreate()\n",
    "print(\"Sess√£o iniciado com sucesso! üöÄ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criar spark dataframe do CSV utilizando o m√©todo read.csv do spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "schema = StructType([\n",
    "  StructField(\"id\", StringType(), True),\n",
    "  StructField(\"text_en\", StringType(), True),\n",
    "  StructField(\"text_pt\", StringType(), True),\n",
    "  StructField(\"sentiment\", StringType(), True),\n",
    "])\n",
    "\n",
    "\n",
    "imdb_df: DataFrame = spark.read.csv('imdb-reviews-pt-br.csv',\n",
    "                         header=True,\n",
    "                         quote=\"\\\"\",\n",
    "                         escape=\"\\\"\",\n",
    "                         encoding=\"UTF-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quest√£o 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criar fun√ß√µes de MAP:\n",
    "- Criar fun√ß√£o para mapear o \"sentiment\" como chave e o \"id\" como valor do tipo inteiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def filter_negative_reviews(data: DataFrame) -> DataFrame:\n",
    "    MEU_RU = \"3803786\"\n",
    "    print(f\"Meu RU √© {MEU_RU}\")\n",
    "\n",
    "    return data.filter(data[\"sentiment\"] == \"neg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cria fun√ß√µes de REDUCE:\n",
    "\n",
    "- Criar fun√ß√£o de reduce para somar os IDs por \"sentiment\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "def sum_negative_ids(reviews: DataFrame) -> DataFrame:\n",
    "  MEU_RU = \"3803786\"\n",
    "  print(f\"Lembrando que meu RU √© {MEU_RU}\")\n",
    "\n",
    "  return reviews.withColumn(\"id\", col(\"id\").cast(\"int\")).select(sum(\"id\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplica√ß√£o do map/reduce e visualiza√ß√£o do resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meu RU √© 3803786\n",
      "Lembrando que meu RU √© 3803786\n",
      "Soma de IDs das reviews negativas: 459568555\n"
     ]
    }
   ],
   "source": [
    "negative_reviews = filter_negative_reviews(imdb_df)\n",
    "sum_of_negative_ids = sum_negative_ids(negative_reviews).collect()[0][0]\n",
    "\n",
    "print(f\"Soma de IDs das reviews negativas: {sum_of_negative_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quest√£o 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criar fun√ß√µes de MAP:\n",
    "- Criar fun√ß√£o para mapear o \"sentiment\" como chave e uma tupla com a soma das palavras de cada texto como valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, split, size\n",
    "\n",
    "def map_sentiment_to_word_count(data: DataFrame) -> DataFrame:\n",
    "  MEU_RU = \"3803786\"\n",
    "  print(f\"Oi! Sou Raquel e meu RU √© {MEU_RU}\")\n",
    "\n",
    "  return data.select(col(\"sentiment\"), size(split(col(\"text_en\"), \"\\\\s+\")).alias(\"text_en_word_count\"), size(split(col(\"text_pt\"), \"\\\\s+\")).alias(\"text_pt_word_count\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cria fun√ß√µes de REDUCE:\n",
    "\n",
    "- Criar fun√ß√£o de reduce para somar o numero de palavras de cada texto portugu√™s e ingl√™s por \"sentiment\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def reduce_word_count_by_sentiment(sentiment_word_counts: DataFrame) -> DataFrame:\n",
    "  MEU_RU = \"3803786\"\n",
    "  print(f\"J√° falei que meu RU √© {MEU_RU}?\")\n",
    "\n",
    "  return sentiment_word_counts.groupBy(\"sentiment\").agg(sum(\"text_en_word_count\").alias(\"total_text_en_words\"), sum(\"text_pt_word_count\").alias(\"total_text_pt_words\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplica√ß√£o do map/reduce e visualiza√ß√£o do resultado\n",
    "\n",
    "1. Aplicar o map/reduce no seu dataframe spark e realizar o collect() ao final\n",
    "2. Selecionar os dados referentes aos textos negativos para realizar a subtra√ß√£o.\n",
    "3. Realizar a subtra√ß√£o das contagens de palavras dos textos negativos para obter o resultado final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meu RU √© 3803786\n",
      "Oi! Sou Raquel e meu RU √© 3803786\n",
      "J√° falei que meu RU √© 3803786?\n",
      "Diferen√ßa entre a contagem de palavras: 54976 (Texto em PT - Texto em EN)\n"
     ]
    }
   ],
   "source": [
    "negative_data = filter_negative_reviews(imdb_df)\n",
    "\n",
    "sentiment_word_counts = map_sentiment_to_word_count(negative_data)\n",
    "total_word_counts = reduce_word_count_by_sentiment(sentiment_word_counts)\n",
    "result = total_word_counts.collect()[0]\n",
    "word_count_difference = result[2] - result[1]\n",
    "\n",
    "print(f\"Diferen√ßa entre a contagem de palavras: {word_count_difference} (Texto em PT - Texto em EN)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
